Fig. 16.1. Conditional pdfs p(vt  vtvt), where vt is the moving 1minute mean, of data of three different days and the standard deviations of conditional pdfs as a function of vt for all 10 daysgrow linearly with vt Fig. 16.1, so that the noise amplitude of the supposed stochastic process is a function of the state (multiplicative noise). In summary, the very simplest model for wind speeds could read vt1  vt  vtt, where 0    1 and  are parameters and t is correlated Gaussian noise. Pre dictions require correlations in data. Different prediction schemes exploit such correlations in different ways. We compare four different predictors which are motivated by the previously discussed properties of the data, written here for predictions k steps ahead;A contiuous state Markov chain of order m assumes that the data representa stochastic process which is fully characterized by conditional probability density functions (cpdf) p(vtkvt, . . . , vtm1), i.e. that the knowledge of measurements which are farther in the past than m steps do not influence the probability distribution of the future values 3, 4. For long range corre lated data such a model is an approximation. The conditional probabilities can be estimated from data as follows Assuming that p(vtkvt, . . . , vtm1) is smooth in the conditioning vector, then the futures of similar sequences of m measurements are drawn from similar distributions. Hence, if we col lect for given time t all past subsequences of length m which are similar to (vt, . . . , vtm1), then their futures form a random sample of the;a stochastic process which is fully characterized by conditional probability density functions (cpdf) p(vtkvt, . . . , vtm1), i.e. that the knowledge of measurements which are farther in the past than m steps do not influence the probability distribution of the future values 3, 4. For long range corre lated data such a model is an approximation. The conditional probabilities can be estimated from data as follows Assuming that p(vtkvt, . . . , vtm1) is smooth in the conditioning vector, then the futures of similar sequences of m measurements are drawn from similar distributions. Hence, if we col lect for given time t all past subsequences of length m which are similar to (vt, . . . , vtm1), then their futures form a random sample of thedesired conditional distribution. This sample can be used to either estimate p(vtkvt, . . . , vtm1) or its moments.;Persistence is expected to be good in cases where increments are uncorrelated and the correlation of the data is strong. The autoregressive model (AR) is expected to be superior if the power spectrum exhibits pronounced peaks. The extrapolation should yield reasonable short time predictions for smooth signals. The Markov chain is able to model a generic nonlinear stochastic